
services:
  backend:
    build:
      context: ./backend
      dockerfile: backend.dockerfile
    ports:
      - "8000:8000"
    environment:
      - UVICORN_RELOAD=true
      - UPLOAD_DIRECTORY=uploaded_files
      - VECTOR_DIRECTORY=vectorstore
      - EMBED_MODEL=nomic-embed-text
      - LLM_MODEL=openhermes
      - OLLAMA_SERVER=http://ollama_server:11434
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
    networks:
      - net

  frontend:
    build:
      context: ./frontend
      dockerfile: frontend.dockerfile
    ports:
      - "3000:3000"
    environment:
      - NEXT_PUBLIC_BACKEND_URL=http://localhost:8000
    networks:
      - net

  ollama_server:
    build:
      context: ./backend
      dockerfile: ollama.dockerfile
    ports:
      - "11434:11434"
    restart: always
    networks:
      - net

volumes:
  ollama_data:
    driver: local

networks:
  net:
    driver: bridge
